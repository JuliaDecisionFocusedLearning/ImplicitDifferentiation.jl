var documenterSearchIndex = {"docs":
[{"location":"examples/0_intro/#Introduction","page":"Introduction","title":"Introduction","text":"We explain the basics of our package on a simple function that is not amenable to naive automatic differentiation.\n\nusing ForwardDiff\nusing ImplicitDifferentiation\nusing LinearAlgebra\nusing Zygote","category":"section"},{"location":"examples/0_intro/#Why-do-we-bother?","page":"Introduction","title":"Why do we bother?","text":"ForwardDiff.jl and Zygote.jl are two prominent packages for automatic differentiation in Julia. While they are very generic, there are simple language constructs that they cannot differentiate through.\n\nfunction badsqrt(x::AbstractArray)\n    a = [0.0]\n    a[1] = x[1]\n    return sqrt.(x)\nend;\nnothing #hide\n\nThis is essentially the componentwise square root function but with an additional twist: a::Vector{Float64} is created internally, and its only element is replaced with the first element of x. We can check that it does what it's supposed to do.\n\nx = [1.0 2.0; 3.0 4.0]\nbadsqrt(x)\n\nOf course the Jacobian has an explicit formula.\n\nJ = Diagonal(0.5 ./ vec(sqrt.(x)))\n\nHowever, things start to go wrong when we compute it with autodiff, due to the limitations of ForwardDiff.jl and those of Zygote.jl.\n\ntry\n    ForwardDiff.jacobian(badsqrt, x)\ncatch e\n    e\nend\n\nForwardDiff.jl throws an error because it tries to call badsqrt with an array of dual numbers, and cannot use one of these numbers to fill a (which has element type Float64).\n\ntry\n    Zygote.jacobian(badsqrt, x)\ncatch e\n    e\nend\n\nZygote.jl also throws an error because it cannot handle mutation.","category":"section"},{"location":"examples/0_intro/#Implicit-function","page":"Introduction","title":"Implicit function","text":"The first possible use of ImplicitDifferentiation.jl is to overcome the limitations of automatic differentiation packages by defining functions (and computing their derivatives) implicitly. An implicit function is a mapping\n\nx in mathbbR^n longmapsto y(x) in mathbbR^m\n\nwhose output is defined by conditions\n\nc(xy(x)) = 0 in mathbbR^m\n\nWe represent it using a type called ImplicitFunction, which you will see in action shortly.\n\nFirst we define a forward mapping corresponding to the function we consider. It returns the actual output y(x) of the function as well as a byproduct z (which can be nothing), and can be thought of as a black box solver. Importantly, this Julia callable doesn't need to be differentiable by automatic differentiation packages but the underlying function still needs to be mathematically differentiable.\n\nforward(x) = (badsqrt(x), nothing);\nnothing #hide\n\nThen we define conditions c(x y z) = 0 that the output y(x) is supposed to satisfy. These conditions must be array-valued, with the same size as y, and they can involve the byproduct z. Unlike the forward mapping, the conditions need to be differentiable by automatic differentiation packages with respect to both x and y. Here the conditions are very obvious: the square of the square root should be equal to the original value.\n\nconditions(x, y, _z) = y .^ 2 .- x\n\nFinally, we construct a wrapper implicit around the previous objects. By default, forward is assumed to return a single output and conditions is assumed to accept 2 arguments.\n\nimplicit = ImplicitFunction(forward, conditions)\n\nWhat does this wrapper do? When we call it as a function, it just falls back on implicit.forward, so unsurprisingly we get the output y(x) and the byproduct z.\n\nimplicit(x)\n\nAnd when we try to compute its Jacobian, the implicit function theorem is applied in the background to circumvent the lack of differentiability of the forward mapping.","category":"section"},{"location":"examples/0_intro/#Forward-and-reverse-mode-autodiff","page":"Introduction","title":"Forward and reverse mode autodiff","text":"Now ForwardDiff.jl works seamlessly.\n\nForwardDiff.jacobian(first ∘ implicit, x) ≈ J\n\nAnd so does Zygote.jl. Hurray!\n\nZygote.jacobian(first ∘ implicit, x)[1] ≈ J\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/3_tricks/#Tricks","page":"Tricks","title":"Tricks","text":"We demonstrate several features that may come in handy for some users.\n\nusing ComponentArrays\nusing ForwardDiff\nusing ImplicitDifferentiation\nusing LinearAlgebra\nusing Zygote","category":"section"},{"location":"examples/3_tricks/#ComponentArrays","page":"Tricks","title":"ComponentArrays","text":"For when you need derivatives with respect to multiple inputs or outputs.\n\nfunction forward_components_aux(a::AbstractVector, b::AbstractVector, m::Number)\n    d = m * sqrt.(a)\n    e = sqrt.(b)\n    return d, e\nend\n\nfunction conditions_components_aux(a, b, m, d, e)\n    c_d = (d ./ m) .^ 2 .- a\n    c_e = (e .^ 2) .- b\n    return c_d, c_e\nend;\nnothing #hide\n\nYou can use ComponentVector from ComponentArrays.jl as an intermediate storage.\n\nfunction forward_components(x::ComponentVector)\n    d, e = forward_components_aux(x.a, x.b, x.m)\n    y = ComponentVector(; d=d, e=e)\n    z = nothing\n    return y, z\nend\n\nfunction conditions_components(x::ComponentVector, y::ComponentVector, _z)\n    c_d, c_e = conditions_components_aux(x.a, x.b, x.m, y.d, y.e)\n    c = ComponentVector(; d=c_d, e=c_e)\n    return c\nend;\nnothing #hide\n\nAnd build your implicit function like so:\n\nimplicit_components = ImplicitFunction(\n    forward_components, conditions_components; strict=Val(false)\n);\nnothing #hide\n\nNow we're good to go.\n\na, b, m = [1.0, 2.0], [3.0, 4.0, 5.0], 6.0\nx = ComponentVector(; a=a, b=b, m=m)\ny, z = implicit_components(x)\n\nAnd it works with both ForwardDiff.jl and Zygote.jl\n\nForwardDiff.jacobian(first ∘ implicit_components, x)\n\nZygote.jacobian(first ∘ implicit_components, x)[1]\n\nfunction full_pipeline(a, b, m)\n    x = ComponentVector(; a=a, b=b, m=m)\n    y, _ = implicit_components(x)\n    return y.d, y.e\nend;\nnothing #hide\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"api/#API-reference","page":"API reference","title":"API reference","text":"","category":"section"},{"location":"api/#Public","page":"API reference","title":"Public","text":"","category":"section"},{"location":"api/#Main-entry-point","page":"API reference","title":"Main entry point","text":"","category":"section"},{"location":"api/#Settings","page":"API reference","title":"Settings","text":"","category":"section"},{"location":"api/#Internals","page":"API reference","title":"Internals","text":"","category":"section"},{"location":"api/#ImplicitDifferentiation","page":"API reference","title":"ImplicitDifferentiation","text":"ImplicitDifferentiation\n\nA Julia package for automatic differentiation of implicit functions.\n\nIts main export is the type ImplicitFunction.\n\n\n\n\n\n","category":"module"},{"location":"api/#ImplicitDifferentiation.ImplicitFunction","page":"API reference","title":"ImplicitDifferentiation.ImplicitFunction","text":"ImplicitFunction\n\nWrapper for an implicit function defined by a solver and a set of conditions which the solution satisfies.\n\nAn ImplicitFunction object behaves like a function, with the following signature:\n\ny, z = (implicit::ImplicitFunction)(x, args...)\n\nThe first output y is differentiable with respect to the first argument x, while the second output z (a byproduct of the solve) and the following positional arguments args are considered constant.\n\nWhen a derivative is queried, the Jacobian of y(x) is computed using the implicit function theorem applied to the conditions c(x, y) (we ignore z for concision):\n\n∂₂c(x, y(x)) * ∂y(x) = -∂₁c(x, y(x))\n\nThis requires solving a linear system A * J = -B where A = ∂₂c, B = ∂₁c and J = ∂y.\n\nConstructor\n\nImplicitFunction(\n    solver,\n    conditions;\n    representation=OperatorRepresentation(),\n    linear_solver=IterativeLinearSolver(),\n    backends=nothing,\n    strict=Val(true),\n)\n\nPositional arguments\n\nsolver: a callable returning (x, args...) -> (y, z) where z is an arbitrary byproduct of the solve. Both x and y must be subtypes of AbstractArray, while z and args can be anything.\nconditions: a callable returning a vector of optimality conditions (x, y, z, args...) -> c, must be compatible with automatic differentiation.\n\nKeyword arguments\n\nrepresentation: defines how the partial Jacobian A of the conditions with respect to the output is represented. It can be either MatrixRepresentation or OperatorRepresentation.\nlinear_solver: specifies how the linear system A * J = -B will be solved in the implicit function theorem. It can be either DirectLinearSolver, IterativeLinearSolver or IterativeLeastSquaresSolver.\nbackends::AbstractADType: specifies how the conditions will be differentiated with respect to x and y. It can be either, nothing, which means that the external autodiff system will be used, or a named tuple (; x=AutoSomething(), y=AutoSomethingElse()) of backend objects from ADTypes.jl.\nstrict::Val: specifies whether preparation inside DifferentiationInterface.jl should enforce a strict match between the primal variables and the provided tangents.\n\n\n\n\n\n","category":"type"},{"location":"api/#ImplicitDifferentiation.MatrixRepresentation","page":"API reference","title":"ImplicitDifferentiation.MatrixRepresentation","text":"MatrixRepresentation\n\nSpecify that the matrix A involved in the implicit function theorem should be represented explicitly, with all its coefficients.\n\nSee also\n\nImplicitFunction\nOperatorRepresentation\n\n\n\n\n\n","category":"type"},{"location":"api/#ImplicitDifferentiation.OperatorRepresentation","page":"API reference","title":"ImplicitDifferentiation.OperatorRepresentation","text":"OperatorRepresentation\n\nSpecify that the matrix A involved in the implicit function theorem should be represented lazily, as a function.\n\nSee also\n\nImplicitFunction\nMatrixRepresentation\n\n\n\n\n\n","category":"type"},{"location":"api/#ImplicitDifferentiation.IterativeLinearSolver","page":"API reference","title":"ImplicitDifferentiation.IterativeLinearSolver","text":"IterativeLinearSolver\n\nSpecify that linear systems Ax = b should be solved with an iterative method.\n\nwarning: Warning\nCan only be used when the solver and the conditions both output AbstractArrays with the same type and length.\n\nSee also\n\nImplicitFunction\nDirectLinearSolver\nIterativeLeastSquaresSolver\n\n\n\n\n\n","category":"type"},{"location":"api/#ImplicitDifferentiation.IterativeLeastSquaresSolver","page":"API reference","title":"ImplicitDifferentiation.IterativeLeastSquaresSolver","text":"IterativeLeastSquaresSolver\n\nSpecify that linear systems Ax = b should be solved with an iterative least-squares method.\n\ntip: Tip\nCan be used when the solver and the conditions output AbstractArrays with different types or different lengths.\n\nwarning: Warning\nTo ensure performance, remember to specify both backends used to differentiate condtions.\n\nSee also\n\nImplicitFunction\nDirectLinearSolver\nIterativeLinearSolver\n\n\n\n\n\n","category":"type"},{"location":"api/#ImplicitDifferentiation.DirectLinearSolver","page":"API reference","title":"ImplicitDifferentiation.DirectLinearSolver","text":"DirectLinearSolver\n\nSpecify that linear systems Ax = b should be solved with a direct method.\n\nwarning: Warning\nCan only be used when the solver and the conditions both output an AbstractVector.\n\nSee also\n\nImplicitFunction\nIterativeLinearSolver\nIterativeLeastSquaresSolver\n\n\n\n\n\n","category":"type"},{"location":"api/#ImplicitDifferentiation.ImplicitFunctionPreparation","page":"API reference","title":"ImplicitDifferentiation.ImplicitFunctionPreparation","text":"ImplicitFunctionPreparation\n\nFields\n\nprep_A: preparation for A (derivative of conditions with respect to y) in forward mode\nprep_Aᵀ: preparation for A (derivative of conditions with respect to y) in reverse mode\nprep_B: preparation for B (derivative of conditions with respect to x) in forward mode\nprep_Bᵀ: preparation for B (derivative of conditions with respect to x) in reverse mode\n\n\n\n\n\n","category":"type"},{"location":"api/#ImplicitDifferentiation.Switch12","page":"API reference","title":"ImplicitDifferentiation.Switch12","text":"Switch12\n\nRepresent a function which behaves like f, except that the first and second arguments are switched:     f(a1, a2, a3) = b becomes     g(a2, a1, a3) = f(a1, a2, a3)\n\n\n\n\n\n","category":"type"},{"location":"api/#ImplicitDifferentiation.prepare_implicit-Union{Tuple{N}, Tuple{ADTypes.AbstractMode, ImplicitFunction, Any, Vararg{Any, N}}} where N","page":"API reference","title":"ImplicitDifferentiation.prepare_implicit","text":"prepare_implicit(\n    mode::ADTypes.AbstractMode,\n    implicit::ImplicitFunction,\n    x_prep,\n    args_prep...;\n    strict=Val(true)\n)\n\nUses the preparation mechanism from DifferentiationInterface.jl to speed up subsequent differentiated calls to implicit(x, args...) where (x, args...) are similar to (x_prep, args_prep...).\n\nThe mode argument is an object from ADTypes.jl that specifies whether the preparation should target ForwardMode, ReverseMode or both (ForwardOrReverseMode).\n\nwarning: Warning\nThis mechanism is not yet part of the public API, use it at your own risk.\n\n\n\n\n\n","category":"method"},{"location":"examples/2_advanced/#Advanced-use-cases","page":"Advanced use cases","title":"Advanced use cases","text":"We dive into more advanced applications of implicit differentiation.\n\nusing ForwardDiff\nusing ImplicitDifferentiation\nusing LinearAlgebra\nusing Optim\nusing Zygote","category":"section"},{"location":"examples/2_advanced/#Constrained-optimization","page":"Advanced use cases","title":"Constrained optimization","text":"First, we show how to differentiate through the solution of a constrained optimization problem:\n\ny(x) = undersety in mathbbR^mmathrmargmin  f(x y) quad textsubject to quad g(x y) leq 0\n\nThe optimality conditions are a bit trickier than in the previous cases. We can projection on the feasible set mathcalC(x) = y g(x y) leq 0  and exploit the convergence of projected gradient descent with step size eta:\n\ny = mathrmproj_mathcalC(x) (y - eta nabla_2 f(x y))\n\nTo make verification easy, we minimize the following objective:\n\nf(x y) = lVert y odot y - x rVert^2\n\non the hypercube mathcalC(x) = 0 1^n. In this case, the optimization problem boils down to a thresholded componentwise square root function, but we implement it using a black box solver from Optim.jl.\n\nfunction forward_cstr_optim(x)\n    f(y) = sum(abs2, y .^ 2 - x)\n    lower = zeros(size(x))\n    upper = ones(size(x))\n    y0 = ones(eltype(x), size(x)) ./ 2\n    res = optimize(f, lower, upper, y0, Fminbox(GradientDescent()))\n    y = Optim.minimizer(res)\n    z = Optim.iterations(res)  # can be useful to retrieve statistics for example\n    return y, z\nend;\nnothing #hide\n\nproj_hypercube(p) = max.(0, min.(1, p))\n\nfunction conditions_cstr_optim(x, y, _z)\n    ∇₂f = @. 4 * (y^2 - x) * y\n    η = 0.1\n    return y .- proj_hypercube(y .- η .* ∇₂f)\nend;\nnothing #hide\n\nWe now have all the ingredients to construct our implicit function.\n\nimplicit_cstr_optim = ImplicitFunction(forward_cstr_optim, conditions_cstr_optim)\n\nAnd indeed, it behaves as it should when we call it:\n\nx = [0.3, 1.4]\n\nThe second component of x is  1, so its square root will be thresholded to one, and the corresponding derivative will be 0.\n\nfirst(implicit_cstr_optim(x)) .^ 2\n\nJ_thres = Diagonal([0.5 / sqrt(x[1]), 0])\n\nForward mode autodiff\n\nForwardDiff.jacobian(first ∘ implicit_cstr_optim, x)\n\nForwardDiff.jacobian(first ∘ forward_cstr_optim, x)\n\nReverse mode autodiff\n\nZygote.jacobian(first ∘ implicit_cstr_optim, x)[1]\n\ntry\n    Zygote.jacobian(first ∘ forward_cstr_optim, x)[1]\ncatch e\n    e\nend\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"#ImplicitDifferentiation.jl","page":"Home","title":"ImplicitDifferentiation.jl","text":"(Image: Stable) (Image: Dev) (Image: Build Status) (Image: Coverage) (Image: Code Style: Blue) (Image: Aqua QA)\n\nImplicitDifferentiation.jl is a package for automatic differentiation of functions defined implicitly, i.e., forward mappings\n\nx in mathbbR^n longmapsto y(x) in mathbbR^m\n\nwhose output is defined by conditions\n\nc(xy(x)) = 0 in mathbbR^m","category":"section"},{"location":"#Background","page":"Home","title":"Background","text":"Implicit differentiation is useful to differentiate through two types of functions:\n\nThose for which automatic differentiation fails. Reasons can vary depending on your backend, but the most common include calls to external solvers, mutating operations or type restrictions.\nThose for which automatic differentiation is very slow. A common example is iterative procedures like fixed point equations or optimization algorithms.\n\nIf you just need a quick overview, check out our JuliaCon 2022 talk. If you want a deeper dive into the theory, you can refer to the paper Efficient and modular implicit differentiation by Blondel et al. (2022).","category":"section"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"To install the stable version, open a Julia REPL and run:\n\nusing Pkg; Pkg.add(\"ImplicitDifferentiation\")\n\nFor the latest version, run this instead:\n\nusing Pkg; Pkg.add(url=\"https://github.com/JuliaDecisionFocusedLearning/ImplicitDifferentiation.jl\")\n\nPlease read the documentation, especially the examples and FAQ.","category":"section"},{"location":"#Related-projects","page":"Home","title":"Related projects","text":"In Julia:\n\nSciML ecosystem, especially LinearSolve.jl, NonlinearSolve.jl and Optimization.jl\njump-dev/DiffOpt.jl: differentiation of convex optimization problems\naxelparmentier/InferOpt.jl: approximate differentiation of combinatorial optimization problems\nJuliaNonconvex/NonconvexUtils.jl: contains the original implementation from which this package drew inspiration\n\nIn Python:\n\ngoogle/jaxopt: hardware accelerated, batchable and differentiable optimizers in JAX","category":"section"},{"location":"faq/#FAQ","page":"FAQ","title":"FAQ","text":"","category":"section"},{"location":"faq/#Supported-autodiff-backends","page":"FAQ","title":"Supported autodiff backends","text":"To differentiate through an ImplicitFunction, the following backends are supported.\n\nBackend Forward mode Reverse mode\nForwardDiff.jl yes -\nChainRules.jl-compatible no yes\nEnzyme.jl soon soon\n\nBy default, the conditions are differentiated using the same \"outer\" backend that is trying to differentiate the ImplicitFunction. However, this can be switched to any other \"inner\" backend compatible with DifferentiationInterface.jl (i.e. a subtype of ADTypes.AbstractADType).","category":"section"},{"location":"faq/#Input-and-output-types","page":"FAQ","title":"Input and output types","text":"","category":"section"},{"location":"faq/#Arrays","page":"FAQ","title":"Arrays","text":"Functions that eat or spit out arbitrary arrays are supported, as long as the forward mapping and conditions return arrays of the same size. The array types involved should be mutable.","category":"section"},{"location":"faq/#Scalars","page":"FAQ","title":"Scalars","text":"Functions that eat or spit out a single number are not supported. The forward mapping and conditions need vectors: instead of returning val you should return [val] (a 1-element Vector).","category":"section"},{"location":"faq/#Number-of-inputs-and-outputs","page":"FAQ","title":"Number of inputs and outputs","text":"Most of the documentation is written for the simple case where the forward mapping is x -> y, i.e. one input and one output. What can you do to handle multiple inputs or outputs? Well, it depends whether you want their derivatives or not.\n\n Derivatives needed Derivatives not needed\nMultiple inputs Make x a ComponentVector Supply args... to forward\nMultiple outputs Make y and c two ComponentVectors Let forward return a nontrivial byproduct z\n\nWe now detail each of these options.","category":"section"},{"location":"faq/#Multiple-inputs-or-outputs-Derivatives-needed","page":"FAQ","title":"Multiple inputs or outputs | Derivatives needed","text":"Say your forward mapping takes multiple inputs and returns multiple outputs, such that you want derivatives for all of them.\n\nThe trick is to leverage ComponentArrays.jl to wrap all the inputs inside a single a ComponentVector, and do the same for all the outputs. See the examples for a demonstration.\n\nwarning: Warning\nYou may run into issues trying to differentiate through the ComponentVector constructor. For instance, Zygote.jl will throw ERROR: Mutating arrays is not supported. Check out this issue for a dirty workaround involving custom chain rules for the constructor.","category":"section"},{"location":"faq/#Multiple-inputs-Derivatives-not-needed","page":"FAQ","title":"Multiple inputs | Derivatives not needed","text":"If your forward mapping (or conditions) takes multiple inputs but you don't care about derivatives, then you can add further positional arguments beyond x. It is important to make sure that the forward mapping and conditions accept the same set of arguments, even if each of these functions only uses a subset of them.\n\nforward(x, arg1, arg2) = y, z\nconditions(x, y, z, arg1, arg2) = c\n\nAll of the positional arguments apart from x will get zero tangents during differentiation of the implicit function.","category":"section"},{"location":"faq/#Multiple-outputs-Derivatives-not-needed","page":"FAQ","title":"Multiple outputs | Derivatives not needed","text":"The last and most tricky situation is when your forward mapping returns multiple outputs, but you only care about some of their derivatives. Then, you need to group the objects you don't want to differentiate into a nontrivial \"byproduct\" z, returned alongside the actual output y. This way, derivatives of z will not be computed: the byproduct is considered constant during differentiation.\n\nThis is mainly useful when the solution procedure creates objects such as Jacobians, which we want to reuse when computing or differentiating the conditions. In that case, you may want to write the conditions differentiation rules yourself. A more advanced application is given by DifferentiableFrankWolfe.jl.","category":"section"},{"location":"faq/#Modeling-tips","page":"FAQ","title":"Modeling tips","text":"","category":"section"},{"location":"faq/#Writing-conditions","page":"FAQ","title":"Writing conditions","text":"We recommend that the conditions themselves do not involve calls to autodiff, even when they describe a gradient. Otherwise, you will need to make sure that nested autodiff works well in your case (i.e. that the \"outer\" backend can differentiate through the \"inner\" backend). For instance, if you're differentiating your implicit function (and your conditions) in reverse mode with Zygote.jl, you may want to use ForwardDiff.jl mode to compute gradients inside the conditions.","category":"section"},{"location":"faq/#Dealing-with-constraints","page":"FAQ","title":"Dealing with constraints","text":"To express constrained optimization problems as implicit functions, you might need differentiable projections or proximal operators to write the optimality conditions. See Efficient and modular implicit differentiation for precise formulations.\n\nIn case these operators are too complicated to code them yourself, here are a few places you can look:\n\nMathOptSetDistances.jl\nProximalOperators.jl\n\nAn alternative is differentiating through the KKT conditions, which is exactly what DiffOpt.jl does for JuMP models.","category":"section"},{"location":"faq/#Memoization","page":"FAQ","title":"Memoization","text":"In some cases, performance might be increased by using memoization to prevent redundant calls to forward. For instance, this is relevant when calculating large Jacobians with forward differentiation, where the computation happens in chunks. Packages such as Memoize.jl and Memoization.jl are useful for defining a memoized version of forward:\n\nusing Memoize\n@memoize Dict forward(x, args...; kwargs...) = y, z","category":"section"},{"location":"examples/1_basic/#Basic-use-cases","page":"Basic use cases","title":"Basic use cases","text":"We show how to differentiate through very common routines:\n\nan unconstrained optimization problem\na nonlinear system of equations\na fixed point iteration\n\nNote that some packages from the SciML ecosystem provide a similar implicit differentiation mechanism.\n\nusing ForwardDiff\nusing ImplicitDifferentiation\nusing LinearAlgebra\nusing NLsolve\nusing Optim\nusing Zygote\n\nIn all three cases, we will use the square root as our forward mapping, but expressed in three different ways. Here's our heroic test vector:\n\nx = [4.0, 9.0];\nnothing #hide\n\nSince we already know the mathematical expression of the Jacobian, we will be able to compare it with our numerical results.\n\nJ = Diagonal(0.5 ./ sqrt.(x))","category":"section"},{"location":"examples/1_basic/#Unconstrained-optimization","page":"Basic use cases","title":"Unconstrained optimization","text":"First, we show how to differentiate through the solution of an unconstrained optimization problem:\n\ny(x) = undersety in mathbbR^mmathrmargmin  f(x y)\n\nThe optimality conditions are given by gradient stationarity:\n\nc(x y) = nabla_2 f(x y) = 0\n\nTo make verification easy, we minimize the following objective:\n\nf(x y) = lVert y odot y - x rVert^2\n\nIn this case, the optimization problem boils down to the componentwise square root function, but we implement it using a black box solver from Optim.jl. Note the presence of an additional positional argument, which is not differentiated.\n\nfunction forward_optim(x, method)\n    f(y) = sum(abs2, y .^ 2 .- x)\n    y0 = ones(eltype(x), size(x))\n    result = optimize(f, y0, method)\n    y = Optim.minimizer(result)\n    z = nothing\n    return y, z\nend;\nnothing #hide\n\nEven though they are defined as a gradient, it is better to provide optimality conditions explicitly: that way we avoid nesting autodiff calls. By default, the conditions should accept two arguments as input. The forward mapping and the conditions should accept the same set of positional arguments.\n\nfunction conditions_optim(x, y, _z, _method)\n    ∇₂f = 4 .* (y .^ 2 .- x) .* y\n    return ∇₂f\nend;\nnothing #hide\n\nWe now have all the ingredients to construct our implicit function.\n\nimplicit_optim = ImplicitFunction(forward_optim, conditions_optim)\n\nAnd indeed, it behaves as it should when we call it:\n\nfirst(implicit_optim(x, LBFGS())) .^ 2\n\nForward mode autodiff\n\nForwardDiff.jacobian(_x -> first(implicit_optim(_x, LBFGS())), x)\n\nIn this instance, we could use ForwardDiff.jl directly on the solver:\n\nForwardDiff.jacobian(_x -> first(forward_optim(_x, LBFGS())), x)\n\nReverse mode autodiff\n\nZygote.jacobian(_x -> first(implicit_optim(_x, LBFGS())), x)[1]\n\nIn this instance, we cannot use Zygote.jl directly on the solver (due to unsupported try/catch statements).\n\ntry\n    Zygote.jacobian(_x -> first(forward_optim(_x, LBFGS())), x)[1]\ncatch e\n    e\nend","category":"section"},{"location":"examples/1_basic/#Nonlinear-system","page":"Basic use cases","title":"Nonlinear system","text":"Next, we show how to differentiate through the solution of a nonlinear system of equations:\n\ntextfind quad y(x) quad textsuch that quad c(x y(x)) = 0\n\nThe optimality conditions are pretty obvious:\n\nc(x y) = 0\n\nTo make verification easy, we solve the following system:\n\nc(x y) = y odot y - x = 0\n\nIn this case, the optimization problem boils down to the componentwise square root function, but we implement it using a black box solver from NLsolve.jl.\n\nfunction forward_nlsolve(x, method)\n    F!(storage, y) = (storage .= y .^ 2 .- x)\n    initial_y = similar(x)\n    initial_y .= 1\n    result = nlsolve(F!, initial_y; method)\n    y = result.zero\n    z = nothing\n    return y, z\nend;\nnothing #hide\n\nfunction conditions_nlsolve(x, y, _z, _method)\n    c = y .^ 2 .- x\n    return c\nend;\nnothing #hide\n\nimplicit_nlsolve = ImplicitFunction(forward_nlsolve, conditions_nlsolve)\n\nfirst(implicit_nlsolve(x, :newton)) .^ 2\n\nForward mode autodiff\n\nForwardDiff.jacobian(_x -> first(implicit_nlsolve(_x, :newton)), x)\n\nForwardDiff.jacobian(_x -> first(forward_nlsolve(_x, :newton)), x)\n\nReverse mode autodiff\n\nZygote.jacobian(_x -> first(implicit_nlsolve(_x, :newton)), x)[1]\n\ntry\n    Zygote.jacobian(_x -> first(forward_nlsolve(_x, :newton)), x)[1]\ncatch e\n    e\nend","category":"section"},{"location":"examples/1_basic/#Fixed-point","page":"Basic use cases","title":"Fixed point","text":"Finally, we show how to differentiate through the limit of a fixed point iteration:\n\ny longmapsto g(x y)\n\nThe optimality conditions are pretty obvious:\n\nc(x y) = g(x y) - y = 0\n\nTo make verification easy, we consider Heron's method:\n\ng(x y) = frac12 left(y + fracxyright)\n\nIn this case, the fixed point algorithm boils down to the componentwise square root function, but we implement it manually.\n\nfunction forward_fixedpoint(x, iterations)\n    y = ones(eltype(x), size(x))\n    for _ in 1:iterations\n        y .= (y .+ x ./ y) ./ 2\n    end\n    z = nothing\n    return y, z\nend;\nnothing #hide\n\nfunction conditions_fixedpoint(x, y, _z, _iterations)\n    g = (y .+ x ./ y) ./ 2\n    return g .- y\nend;\nnothing #hide\n\nimplicit_fixedpoint = ImplicitFunction(forward_fixedpoint, conditions_fixedpoint)\n\nfirst(implicit_fixedpoint(x, 10)) .^ 2\n\nForward mode autodiff\n\nForwardDiff.jacobian(_x -> first(implicit_fixedpoint(_x, 10)), x)\n\nForwardDiff.jacobian(_x -> first(forward_fixedpoint(_x, 10)), x)\n\nReverse mode autodiff\n\nZygote.jacobian(_x -> first(implicit_fixedpoint(_x, 10)), x)[1]\n\ntry\n    Zygote.jacobian(_x -> first(forward_fixedpoint(_x, 10)), x)[1]\ncatch e\n    e\nend\n\n\n\nThis page was generated using Literate.jl.","category":"section"}]
}
