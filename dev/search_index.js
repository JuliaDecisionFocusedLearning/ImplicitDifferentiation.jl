var documenterSearchIndex = {"docs":
[{"location":"api/#API-reference","page":"API reference","title":"API reference","text":"","category":"section"},{"location":"api/#Index","page":"API reference","title":"Index","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"","category":"page"},{"location":"api/#Docstrings","page":"API reference","title":"Docstrings","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"Modules = [ImplicitDifferentiation]","category":"page"},{"location":"api/#ImplicitDifferentiation.ImplicitFunction","page":"API reference","title":"ImplicitDifferentiation.ImplicitFunction","text":"ImplicitFunction{F,C,L}\n\nDifferentiable wrapper for an implicit function x -> ŷ(x) whose output is defined by explicit conditions F(x,ŷ(x)) = 0.\n\nWe can obtain the Jacobian of ŷ with the implicit function theorem:\n\n∂₁F(x,ŷ(x)) + ∂₂F(x,ŷ(x)) * ∂ŷ(x) = 0\n\nIf x ∈ ℝⁿ, y ∈ ℝᵐ and F(x,y) ∈ ℝᶜ, this amounts to solving the linear system A * J = B, where A ∈ ℝᶜᵐ, B ∈ ℝᶜⁿ and J ∈ ℝᵐⁿ.\n\nFields:\n\nforward::F: callable of the form x -> ŷ(x)\nconditions::C: callable of the form (x,y) -> F(x,y)\nlinear_solver::L: callable of the form (A,b) -> u such that A * u = b\n\n\n\n\n\n","category":"type"},{"location":"api/#ImplicitDifferentiation.ImplicitFunction-Tuple{Any}","page":"API reference","title":"ImplicitDifferentiation.ImplicitFunction","text":"implicit(x)\n\nMake ImplicitFunction{F,C,L} callable by applying implicit.forward.\n\n\n\n\n\n","category":"method"},{"location":"api/#ImplicitDifferentiation.ImplicitFunction-Union{Tuple{C}, Tuple{F}, Tuple{F, C}} where {F, C}","page":"API reference","title":"ImplicitDifferentiation.ImplicitFunction","text":"ImplicitFunction(forward, conditions)\n\nConstruct an ImplicitFunction{F,C,L} with Krylov.gmres as the default linear solver.\n\n\n\n\n\n","category":"method"},{"location":"api/#ChainRulesCore.frule-Union{Tuple{R}, Tuple{ChainRulesCore.RuleConfig, Any, ImplicitFunction, AbstractArray{R}}} where R<:Real","page":"API reference","title":"ChainRulesCore.frule","text":"frule(rc, (_, dx), implicit, x)\n\nCustom forward rule for ImplicitFunction{F,C,L}.\n\nWe compute the Jacobian-vector product Jv by solving Au = Bv and setting Jv = u.\n\n\n\n\n\n","category":"method"},{"location":"api/#ChainRulesCore.rrule-Union{Tuple{R}, Tuple{ChainRulesCore.RuleConfig, ImplicitFunction, AbstractArray{R}}} where R<:Real","page":"API reference","title":"ChainRulesCore.rrule","text":"rrule(rc, implicit, x)\n\nCustom reverse rule for ImplicitFunction{F,C,L}.\n\nWe compute the vector-Jacobian product Jᵀv by solving Aᵀu = v and setting Jᵀv = Bᵀu.\n\n\n\n\n\n","category":"method"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"EditURL = \"https://github.com/gdalle/ImplicitDifferentiation.jl/blob/main/test/2_sparse_linear_regression.jl\"","category":"page"},{"location":"examples/2_sparse_linear_regression/#Sparse-linear-regression","page":"Sparse linear regression","title":"Sparse linear regression","text":"","category":"section"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"In this example, we show how to differentiate through the solution of the following constrained optimization problem:","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"haty(x) = min_y in mathcalC f(x y)","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"where mathcalC is a closed convex set. The optimal solution can be found as the fixed point of the projected gradient algorithm for any step size eta. This insight yields the following optimality conditions:","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"F(x haty(x)) = 0 quad textwith quad F(xy) = mathrmproj_mathcalC(y - eta nabla_2 f(x y)) - y","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"using ComponentArrays\nusing Convex\nusing FiniteDifferences\nusing ImplicitDifferentiation\nusing MathOptInterface\nusing MathOptSetDistances\nusing Random\nusing SCS\nusing Zygote\n\n\nRandom.seed!(63)","category":"page"},{"location":"examples/2_sparse_linear_regression/#Introduction","page":"Sparse linear regression","title":"Introduction","text":"","category":"section"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"We have a matrix of features X in mathbbR^n times p and a vector of targets y in mathbbR^n.","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"In a linear regression setting y approx X beta, one way to ensure sparsity of the parameter beta in mathbbR^p is to select it within the ell_1 ball mathcalB_1:","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"hatbeta(X y) = min_beta  lVert y - X beta rVert_2^2 quad textst quad lVert beta rVert_1 leq 1 tagQP","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"We want to compute the derivatives of the optimal parameter wrt to the data: partial hatbeta  partial X and partial hatbeta  partial y.","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"Possible application: sensitivity analysis of hatbeta(X y).","category":"page"},{"location":"examples/2_sparse_linear_regression/#Forward-solver","page":"Sparse linear regression","title":"Forward solver","text":"","category":"section"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"The function hatbeta is computed with a disciplined convex solver thanks to Convex.jl.","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"function lasso(X::AbstractMatrix, y::AbstractVector)\n\tn, p = size(X)\n\tβ = Variable(p)\n\tobjective = sumsquares(X * β - y)\n\tconstraints = [norm(β, 1) <= 1.]\n\tproblem = minimize(objective, constraints)\n\tsolve!(problem, SCS.Optimizer; silent_solver=true)\n\treturn Convex.evaluate(β)\nend","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"To comply with the requirements of ImplicitDifferentiation.jl, we need to provide the input arguments within a single array. We exploit ComponentArrays.jl for that purpose.","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"lasso(data::ComponentVector) = lasso(data.X, data.y)","category":"page"},{"location":"examples/2_sparse_linear_regression/#Optimality-conditions","page":"Sparse linear regression","title":"Optimality conditions","text":"","category":"section"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"We use MathOptSetDistances.jl to compute the projection onto the unit ell_1 ball.","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"function proj_l1_ball(v::AbstractVector{R}) where {R<:Real}\n\tdistance = MathOptSetDistances.DefaultDistance()\n    cone = MathOptInterface.NormOneCone(length(v))\n\tball = MathOptSetDistances.NormOneBall{R}(one(R), cone)\n\treturn projection_on_set(distance, v, ball)\nend","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"Since this projection uses mutation internally, it is not compatible with Zygote.jl. Thus, we need to specify that it should be differentiated with ForwardDiff.jl.","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"function proj_grad_fixed_point(data, β)\n\tgrad = 2 * data.X' * (data.X * β - data.y)\n\treturn β - Zygote.forwarddiff(proj_l1_ball, β - grad)\nend","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"This is the last ingredient we needed to build a differentiable sparse linear regression.","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"implicit = ImplicitFunction(lasso, proj_grad_fixed_point);\nnothing #hide","category":"page"},{"location":"examples/2_sparse_linear_regression/#Testing","page":"Sparse linear regression","title":"Testing","text":"","category":"section"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"n, p = 5, 7;\nX, y = rand(n, p), rand(n);\ndata = ComponentVector(X=X, y=y);\nnothing #hide","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"As expected, the forward pass returns a sparse solution","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"round.(implicit(data); digits=4)","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"Note that implicit differentiation is necessary here because the convex solver breaks autodiff.","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"try; Zygote.jacobian(lasso, data); catch e; @error e; end","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"Meanwhile, our implicit wrapper makes autodiff work seamlessly.","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"J = Zygote.jacobian(implicit, data)[1]","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"The number of columns of the Jacobian is explained by the following formula:","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"prod(size(X)) + prod(size(y))","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"We can validate the result using finite differences.","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"J_ref = FiniteDifferences.jacobian(central_fdm(5, 1), lasso, data)[1]\nsum(abs, J - J_ref) / prod(size(J))","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"","category":"page"},{"location":"examples/2_sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"This page was generated using Literate.jl.","category":"page"},{"location":"background/#Mathematical-background","page":"Mathematical background","title":"Mathematical background","text":"","category":"section"},{"location":"background/","page":"Mathematical background","title":"Mathematical background","text":"warning: Work in progress\nIn the meantime, please refer to the preprint Efficient and modular implicit differentiation for an introduction to the methods implemented here.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = ImplicitDifferentiation","category":"page"},{"location":"#ImplicitDifferentiation.jl","page":"Home","title":"ImplicitDifferentiation.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ImplicitDifferentiation.jl is a package for automatic differentiation of implicit functions.","category":"page"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the stable version, open a Julia REPL and run:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Pkg; Pkg.add(\"ImplicitDifferentiation\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"For the latest version, run this instead:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Pkg; Pkg.add(url=\"https://github.com/gdalle/ImplicitDifferentiation.jl\")","category":"page"},{"location":"#Related-packages","page":"Home","title":"Related packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DiffOpt.jl: differentiation of convex optimization problems\nInferOpt.jl: differentiation of combinatorial optimization problems\nNonconvexUtils.jl: contains the original implementation from which this package drew inspiration","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"EditURL = \"https://github.com/gdalle/ImplicitDifferentiation.jl/blob/main/test/1_unconstrained_optimization.jl\"","category":"page"},{"location":"examples/1_unconstrained_optimization/#Unconstrained-optimization","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"","category":"section"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"In this example, we show how to differentiate through the solution of the following unconstrained optimization problem:","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"haty(x) = min_y in mathbbR^m f(x y)","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"The optimality conditions are given by gradient stationarity:","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"F(x haty(x)) = 0 quad textwith quad F(xy) = nabla_2 f(x y) = 0","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"using ImplicitDifferentiation\nusing Optim\nusing Random\nusing Zygote\n\n\nRandom.seed!(63)","category":"page"},{"location":"examples/1_unconstrained_optimization/#Implicit-function-wrapper","page":"Unconstrained optimization","title":"Implicit function wrapper","text":"","category":"section"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"To make verification easy, we minimize a quadratic objective","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"f(x y) = lVert y - x rVert^2","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"In this case, the optimization algorithm is very simple (the identity function does the job), but still we implement it using a black box solver from Optim.jl to show that it doesn't change the result.","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"function dumb_identity(x)\n    f(y) = sum(abs2, y-x)\n    y0 = zero(x)\n    res = optimize(f, y0, LBFGS(); autodiff=:forward)\n    y = Optim.minimizer(res)\n    return y\nend;\nnothing #hide","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"On the other hand, optimality conditions should be provided explicitly whenever possible, so as to avoid nesting autodiff calls.","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"zero_gradient(x, y) = 2(y - x);\nnothing #hide","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"We now have all the ingredients to construct our implicit function.","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"implicit = ImplicitFunction(dumb_identity, zero_gradient);\nnothing #hide","category":"page"},{"location":"examples/1_unconstrained_optimization/#Testing","page":"Unconstrained optimization","title":"Testing","text":"","category":"section"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"x = rand(3, 2)","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"Let's start by taking a look at the forward pass, which should be the identity function.","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"implicit(x)","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"We now check whether the behavior of our ImplicitFunction wrapper is coherent with the theoretical derivatives.","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"Zygote.jacobian(implicit, x)[1]","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"As expected, we recover the identity matrix as Jacobian. Strictly speaking, the Jacobian should be a 4D tensor, but it is flattened by Zygote into a 2D matrix.","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"Note that implicit differentiation was necessary here, since our solver alone doesn't support autodiff with Zygote.jl.","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"try; Zygote.jacobian(dumb_identity, x)[1]; catch e; @error e; end","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"","category":"page"},{"location":"examples/1_unconstrained_optimization/","page":"Unconstrained optimization","title":"Unconstrained optimization","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"EditURL = \"https://github.com/gdalle/ImplicitDifferentiation.jl/blob/main/test/3_optimal_transport.jl\"","category":"page"},{"location":"examples/3_optimal_transport/#Optimal-transport","page":"Optimal transport","title":"Optimal transport","text":"","category":"section"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"In this example, we show how to differentiate through the solution of the entropy-regularized optimal transport problem.","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"using Distances\nusing FiniteDifferences\nusing ImplicitDifferentiation\nusing Random\nusing Zygote\n\n\nRandom.seed!(63)","category":"page"},{"location":"examples/3_optimal_transport/#Introduction","page":"Optimal transport","title":"Introduction","text":"","category":"section"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"Here we give a brief introduction to optimal transport, see the book by Gabriel Peyré and Marco Cuturi for more details.","category":"page"},{"location":"examples/3_optimal_transport/#Problem-description","page":"Optimal transport","title":"Problem description","text":"","category":"section"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"Suppose we have a distribution of mass a in Delta^n over points x_1  x_n in mathbbR^d (where Delta denotes the probability simplex). We want to transport it to a distribution b in Delta^m over points y_1  y_m in mathbbR^d. The unit moving cost from x to y is proportional to the squared Euclidean distance c(x y) = lVert x - y rVert_2^2.","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"A transportation plan can be described by a coupling p = Pi(a b), i.e. a probability distribution on the product space with the right marginals:","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"Pi(a b) = p in Delta^n times m p mathbf1 = a p^top mathbf1 = b","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"Let C in mathbbR^n times m be the moving cost matrix, with C_ij = c(x_i y_j). The basic optimization problem we want to solve is a linear program:","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"hatp(C) = min_p in Pi(a b) sum_ij p_ij C_ij","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"In order to make it smoother, we add an entropic regularization term:","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"hatp_varepsilon(C) = min_p in Pi(a b) sum_ij left(p_ij C_ij + varepsilon p_ij log fracp_ija_i b_j right)","category":"page"},{"location":"examples/3_optimal_transport/#Sinkhorn-algorithm","page":"Optimal transport","title":"Sinkhorn algorithm","text":"","category":"section"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"To solve the regularized problem, we can use the Sinkhorn fixed point algorithm. Let K in mathbbR^n times m be the matrix defined by K_ij = exp(-C_ij  varepsilon). Then the optimal coupling hatp_varepsilon(C) can be written as:","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"hatp_varepsilon(C) = mathrmdiag(hatu)  K  mathrmdiag(hatv) tag1","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"where hatu and hatv are the fixed points of the following Sinkhorn iteration:","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"u^t+1 = fracaKv^t qquad textand qquad v^t+1 = fracbK^top u^t tagS","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"The implicit function theorem can be used to differentiate hatu and hatv with respect to C, a and/or b. This can be combined with automatic differentiation of Equation (1) to find the Jacobian","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"J = fracpartial  mathrmvec(hatp_varepsilon(C))partial  mathrmvec(C)","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"d = 10\nn = 3\nm = 4\n\nX = rand(d, n)\nY = rand(d, m)\n\na = fill(1 / n, n)\nb = fill(1 / m, m)\nC = pairwise(SqEuclidean(), X, Y, dims=2)\n\nε = 1.;\nnothing #hide","category":"page"},{"location":"examples/3_optimal_transport/#Forward-solver","page":"Optimal transport","title":"Forward solver","text":"","category":"section"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"For technical reasons related to optimality checking, our Sinkhorn solver returns hatu instead of hatp_varepsilon.","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"function sinkhorn(C; a=a, b=b, ε=ε)\n    K = exp.(.-C ./ ε)\n    u = copy(a)\n    v = copy(b)\n    for t in 1:100\n        u .= a ./ (K * v)\n        v .= b ./ (K' * u)\n    end\n    return u\nend","category":"page"},{"location":"examples/3_optimal_transport/#Optimality-conditions","page":"Optimal transport","title":"Optimality conditions","text":"","category":"section"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"We simply used the fixed point equation (textS).","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"function sinkhorn_fixed_point(C, u; a=a, b=b, ε=ε)\n    K = exp.(.-C ./ ε)\n    v = b ./ (K' * u)\n    return u .- a ./ (K * v)\nend","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"We have all we need to build a differentiable Sinkhorn that doesn't require unrolling the fixed point iterations.","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"implicit = ImplicitFunction(sinkhorn, sinkhorn_fixed_point);\nnothing #hide","category":"page"},{"location":"examples/3_optimal_transport/#Testing","page":"Optimal transport","title":"Testing","text":"","category":"section"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"u = sinkhorn(C)","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"First, let us check that the forward pass works correctly and returns a fixed point.","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"maximum(abs, sinkhorn_fixed_point(C, u))","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"Using the implicit function defined above, we can build an autodiff-compatible implementation of transportation_plan which does not require backpropagating through the Sinkhorn iterations:","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"function transportation_plan(C; a=a, b=b, ε=ε)\n    K = exp.(.-C ./ ε)\n    u = implicit(C)\n    v = b ./ (K' * u)\n    p_vec = vec(u .* K .* v')\n    return p_vec\nend;\nnothing #hide","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"Let us compare its Jacobian with the one obtained using finite differences.","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"J = Zygote.jacobian(transportation_plan, C)[1]\nJ_ref = FiniteDifferences.jacobian(central_fdm(5, 1), transportation_plan, C)[1]\nsum(abs, J - J_ref) / prod(size(J))","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"","category":"page"},{"location":"examples/3_optimal_transport/","page":"Optimal transport","title":"Optimal transport","text":"This page was generated using Literate.jl.","category":"page"}]
}
