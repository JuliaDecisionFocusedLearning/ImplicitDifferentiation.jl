<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Sparse linear regression · ImplicitDifferentiation.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://gdalle.github.io/ImplicitDifferentiation.jl/examples/2_sparse_linear_regression/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">ImplicitDifferentiation.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../background/">Mathematical background</a></li><li><a class="tocitem" href="../../api/">API reference</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../1_unconstrained_optimization/">Unconstrained optimization</a></li><li class="is-active"><a class="tocitem" href>Sparse linear regression</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#Forward-solver"><span>Forward solver</span></a></li><li><a class="tocitem" href="#Optimality-conditions"><span>Optimality conditions</span></a></li><li><a class="tocitem" href="#Testing"><span>Testing</span></a></li></ul></li><li><a class="tocitem" href="../3_optimal_transport/">Optimal transport</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Sparse linear regression</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Sparse linear regression</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/gdalle/ImplicitDifferentiation.jl/blob/main/test/2_sparse_linear_regression.jl" title="View on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">View on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Sparse-linear-regression"><a class="docs-heading-anchor" href="#Sparse-linear-regression">Sparse linear regression</a><a id="Sparse-linear-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Sparse-linear-regression" title="Permalink"></a></h1><p>In this example, we show how to differentiate through the solution of the following constrained optimization problem:</p><p class="math-container">\[\hat{y}(x) = \min_{y \in \mathcal{C}} f(x, y)\]</p><p>where <span>$\mathcal{C}$</span> is a closed convex set. The optimal solution can be found as the fixed point of the projected gradient algorithm for any step size <span>$\eta$</span>. This insight yields the following optimality conditions:</p><p class="math-container">\[F(x, \hat{y}(x)) = 0 \quad \text{with} \quad F(x,y) = \mathrm{proj}_{\mathcal{C}}(y - \eta \nabla_2 f(x, y)) - y\]</p><pre><code class="language-julia hljs">using ComponentArrays
using Convex
using FiniteDifferences
using ImplicitDifferentiation
using MathOptInterface
using MathOptSetDistances
using Random
using SCS
using Zygote


Random.seed!(63)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Random.TaskLocalRNG()</code></pre><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><p>We have a matrix of features <span>$X \in \mathbb{R}^{n \times p}$</span> and a vector of targets <span>$y \in \mathbb{R}^n$</span>.</p><p>In a linear regression setting <span>$y \approx X \beta$</span>, one way to ensure sparsity of the parameter <span>$\beta \in \mathbb{R}^p$</span> is to select it within the <span>$\ell_1$</span> ball <span>$\mathcal{B}_1$</span>:</p><p class="math-container">\[\hat{\beta}(X, y) = \min_{\beta} ~ \lVert y - X \beta \rVert_2^2 \quad \text{s.t.} \quad \lVert \beta \rVert_1 \leq 1 \tag{QP}\]</p><p>We want to compute the derivatives of the optimal parameter wrt to the data: <span>$\partial \hat{\beta} / \partial X$</span> and <span>$\partial \hat{\beta} / \partial y$</span>.</p><p>Possible application: sensitivity analysis of <span>$\hat{\beta}(X, y)$</span>.</p><h2 id="Forward-solver"><a class="docs-heading-anchor" href="#Forward-solver">Forward solver</a><a id="Forward-solver-1"></a><a class="docs-heading-anchor-permalink" href="#Forward-solver" title="Permalink"></a></h2><p>The function <span>$\hat{\beta}$</span> is computed with a disciplined convex solver thanks to <code>Convex.jl</code>.</p><pre><code class="language-julia hljs">function lasso(X::AbstractMatrix, y::AbstractVector)
	n, p = size(X)
	β = Variable(p)
	objective = sumsquares(X * β - y)
	constraints = [norm(β, 1) &lt;= 1.]
	problem = minimize(objective, constraints)
	solve!(problem, SCS.Optimizer; silent_solver=true)
	return Convex.evaluate(β)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">lasso (generic function with 1 method)</code></pre><p>To comply with the requirements of <code>ImplicitDifferentiation.jl</code>, we need to provide the input arguments within a single array. We exploit <code>ComponentArrays.jl</code> for that purpose.</p><pre><code class="language-julia hljs">lasso(data::ComponentVector) = lasso(data.X, data.y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">lasso (generic function with 2 methods)</code></pre><h2 id="Optimality-conditions"><a class="docs-heading-anchor" href="#Optimality-conditions">Optimality conditions</a><a id="Optimality-conditions-1"></a><a class="docs-heading-anchor-permalink" href="#Optimality-conditions" title="Permalink"></a></h2><p>We use <code>MathOptSetDistances.jl</code> to compute the projection onto the unit <span>$\ell_1$</span> ball.</p><pre><code class="language-julia hljs">function proj_l1_ball(v::AbstractVector{R}) where {R&lt;:Real}
	distance = MathOptSetDistances.DefaultDistance()
    cone = MathOptInterface.NormOneCone(length(v))
	ball = MathOptSetDistances.NormOneBall{R}(one(R), cone)
	return projection_on_set(distance, v, ball)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">proj_l1_ball (generic function with 1 method)</code></pre><p>Since this projection uses mutation internally, it is not compatible with <code>Zygote.jl</code>. Thus, we need to specify that it should be differentiated with <code>ForwardDiff.jl</code>.</p><pre><code class="language-julia hljs">function proj_grad_fixed_point(data, β)
	grad = 2 * data.X&#39; * (data.X * β - data.y)
	return β - Zygote.forwarddiff(proj_l1_ball, β - grad)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">proj_grad_fixed_point (generic function with 1 method)</code></pre><p>This is the last ingredient we needed to build a differentiable sparse linear regression.</p><pre><code class="language-julia hljs">implicit = ImplicitFunction(lasso, proj_grad_fixed_point);</code></pre><h2 id="Testing"><a class="docs-heading-anchor" href="#Testing">Testing</a><a id="Testing-1"></a><a class="docs-heading-anchor-permalink" href="#Testing" title="Permalink"></a></h2><pre><code class="language-julia hljs">n, p = 5, 7;
X, y = rand(n, p), rand(n);
data = ComponentVector(X=X, y=y);</code></pre><p>As expected, the forward pass returns a sparse solution</p><pre><code class="language-julia hljs">round.(implicit(data); digits=4)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">7-element Vector{Float64}:
 0.3643
 0.0506
 0.0
 0.377
 0.2081
 0.0
 0.0</code></pre><p>Note that implicit differentiation is necessary here because the convex solver breaks autodiff.</p><pre><code class="language-julia hljs">try; Zygote.jacobian(lasso, data); catch e; @error e; end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Error: MethodError(zero, (+ (affine; real)
│ ├─ * (affine; real)
│ │  ├─ 5×7 reshape(view(::Vector{Float64}, 1:35), 5, 7) with eltype Float64
│ │  └─ 7-element real variable (id: 142…951)
│ └─ 5-element Vector{Float64},), 0x0000000000007b42)
└ @ Main 2_sparse_linear_regression.md:113</code></pre><p>Meanwhile, our implicit wrapper makes autodiff work seamlessly.</p><pre><code class="language-julia hljs">J = Zygote.jacobian(implicit, data)[1]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">7×40 Matrix{Float64}:
 -0.501362  -0.745416   0.804222  …   1.28638     0.101484  -0.906656
  0.426837   1.13999   -1.00262      -1.50601     0.985644   0.388479
  0.0        0.0        0.0           0.0         0.0        0.0
  0.193122   0.193906  -0.313604      0.130775   -0.902003   0.355226
 -0.118597  -0.588484   0.512001      0.0888583  -0.185125   0.16295
  0.0        0.0        0.0       …   0.0         0.0        0.0
  0.0        0.0        0.0           0.0         0.0        0.0</code></pre><p>The number of columns of the Jacobian is explained by the following formula:</p><pre><code class="language-julia hljs">prod(size(X)) + prod(size(y))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">40</code></pre><p>We can validate the result using finite differences.</p><pre><code class="language-julia hljs">J_ref = FiniteDifferences.jacobian(central_fdm(5, 1), lasso, data)[1]
sum(abs, J - J_ref) / prod(size(J))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.0005758208253650121</code></pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../1_unconstrained_optimization/">« Unconstrained optimization</a><a class="docs-footer-nextpage" href="../3_optimal_transport/">Optimal transport »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.19 on <span class="colophon-date" title="Wednesday 29 June 2022 11:07">Wednesday 29 June 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
